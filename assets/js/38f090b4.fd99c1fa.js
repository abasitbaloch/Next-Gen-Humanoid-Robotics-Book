"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[285],{553:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var s=t(4848),a=t(8453);const i={sidebar_position:6},r="Human-Robot Interaction",o={id:"module-4-vla/human-robot-interaction",title:"Human-Robot Interaction",description:"Overview",source:"@site/docs/docs/module-4-vla/human-robot-interaction.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/human-robot-interaction",permalink:"/docs/module-4-vla/human-robot-interaction",draft:!1,unlisted:!1,editUrl:"https://github.com/abasitbaloch/Next-Gen-Humanoid-Robotics-Book/tree/main/docs/docs/module-4-vla/human-robot-interaction.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Autonomous Behaviors",permalink:"/docs/module-4-vla/autonomous-behaviors"},next:{title:"Capstone: Autonomous Humanoid Integration",permalink:"/docs/capstone/intro"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"HRI Architecture",id:"hri-architecture",level:2},{value:"Interaction Modalities",id:"interaction-modalities",level:3},{value:"Interaction Framework",id:"interaction-framework",level:3},{value:"Voice Interaction",id:"voice-interaction",level:2},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Context-Aware Responses",id:"context-aware-responses",level:3},{value:"Non-Verbal Communication",id:"non-verbal-communication",level:2},{value:"Gesture Recognition",id:"gesture-recognition",level:3},{value:"Expressive Motion",id:"expressive-motion",level:3},{value:"Social Interaction Patterns",id:"social-interaction-patterns",level:2},{value:"Turn-Taking",id:"turn-taking",level:3},{value:"Proxemics",id:"proxemics",level:3},{value:"Personalization and Adaptation",id:"personalization-and-adaptation",level:2},{value:"User Profiling",id:"user-profiling",level:3},{value:"Safety and Comfort",id:"safety-and-comfort",level:2},{value:"Safety Protocols",id:"safety-protocols",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:2},{value:"Coordinated Interaction",id:"coordinated-interaction",level:3},{value:"Cultural Considerations",id:"cultural-considerations",level:2},{value:"Cultural Adaptation",id:"cultural-adaptation",level:3},{value:"Privacy and Ethics",id:"privacy-and-ethics",level:2},{value:"Privacy Protection",id:"privacy-protection",level:3},{value:"Evaluation and Metrics",id:"evaluation-and-metrics",level:2},{value:"Interaction Quality Metrics",id:"interaction-quality-metrics",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Solutions",id:"solutions",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This section covers Human-Robot Interaction (HRI) systems that enable natural, intuitive communication between humans and humanoid robots. Effective HRI is crucial for robots to work safely and productively alongside humans in various environments."}),"\n",(0,s.jsx)(n.h2,{id:"hri-architecture",children:"HRI Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"interaction-modalities",children:"Interaction Modalities"}),"\n",(0,s.jsx)(n.p,{children:"HRI systems typically support multiple interaction modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice"}),": Natural language communication"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gesture"}),": Visual communication through body language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Touch"}),": Physical interaction and haptic feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual"}),": Displays, lights, and visual feedback"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Motion"}),": Robot movement expressing intent"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"interaction-framework",children:"Interaction Framework"}),"\n",(0,s.jsx)(n.p,{children:"The HRI framework includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perception"}),": Understanding human input and state"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Interpretation"}),": Making sense of human communication"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Generation"}),": Creating appropriate robot responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feedback"}),": Providing clear feedback to humans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adaptation"}),": Learning and adapting to human preferences"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"voice-interaction",children:"Voice Interaction"}),"\n",(0,s.jsx)(n.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,s.jsx)(n.p,{children:"Implement natural language processing for HRI:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\nimport pyttsx3\nimport spacy\nfrom typing import Dict, List, Tuple\n\nclass VoiceInteractionManager:\n    def __init__(self):\n        # Initialize speech recognition\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Initialize text-to-speech\n        self.tts_engine = pyttsx3.init()\n        self.setup_tts()\n\n        # Initialize NLP\n        self.nlp = spacy.load(\"en_core_web_sm\")\n\n        # Interaction context\n        self.conversation_history = []\n        self.user_preferences = {}\n\n    def setup_tts(self):\n        \"\"\"Configure text-to-speech parameters\"\"\"\n        voices = self.tts_engine.getProperty('voices')\n        self.tts_engine.setProperty('rate', 150)  # Words per minute\n        self.tts_engine.setProperty('volume', 0.8)\n\n        # Choose voice based on robot personality\n        for voice in voices:\n            if \"female\" in voice.name.lower():\n                self.tts_engine.setProperty('voice', voice.id)\n                break\n\n    def listen_and_understand(self) -> Dict[str, any]:\n        \"\"\"Listen to human input and understand intent\"\"\"\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n            audio = self.recognizer.listen(source)\n\n        try:\n            text = self.recognizer.recognize_google(audio)\n            intent = self.parse_intent(text)\n\n            return {\n                'text': text,\n                'intent': intent,\n                'confidence': 0.9,  # Would be calculated in real implementation\n                'timestamp': time.time()\n            }\n        except sr.UnknownValueError:\n            return {'error': 'Could not understand audio'}\n        except sr.RequestError as e:\n            return {'error': f'Service error: {e}'}\n\n    def parse_intent(self, text: str) -> Dict[str, any]:\n        \"\"\"Parse intent from natural language text\"\"\"\n        doc = self.nlp(text)\n\n        # Extract entities and dependencies\n        entities = [(ent.text, ent.label_) for ent in doc.ents]\n        verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n\n        # Determine intent based on patterns\n        intent = self.classify_intent(text, verbs, entities)\n\n        return {\n            'type': intent,\n            'entities': entities,\n            'verbs': verbs,\n            'raw_text': text\n        }\n\n    def classify_intent(self, text: str, verbs: List[str], entities: List[Tuple[str, str]]) -> str:\n        \"\"\"Classify the intent of the input\"\"\"\n        text_lower = text.lower()\n\n        # Intent classification rules\n        if any(word in text_lower for word in ['hello', 'hi', 'hey', 'greetings']):\n            return 'greeting'\n        elif any(word in text_lower for word in ['help', 'assist', 'support']):\n            return 'request_help'\n        elif any(word in text_lower for word in ['move', 'go', 'navigate', 'walk']):\n            return 'navigation_request'\n        elif any(word in text_lower for word in ['grasp', 'pick', 'take', 'get']):\n            return 'manipulation_request'\n        elif any(word in text_lower for word in ['find', 'look', 'search']):\n            return 'perception_request'\n        else:\n            return 'unknown'\n"})}),"\n",(0,s.jsx)(n.h3,{id:"context-aware-responses",children:"Context-Aware Responses"}),"\n",(0,s.jsx)(n.p,{children:"Generate contextually appropriate responses:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ContextualResponseGenerator:\n    def __init__(self):\n        self.response_templates = {\n            \'greeting\': [\n                "Hello! How can I assist you today?",\n                "Greetings! I\'m ready to help.",\n                "Hi there! What would you like me to do?"\n            ],\n            \'request_help\': [\n                "I\'m here to help. You can ask me to perform tasks like \'move to the kitchen\' or \'find the red cup\'.",\n                "I can assist with navigation, object manipulation, and perception tasks. What do you need?",\n                "I\'m ready to help! Please tell me what you\'d like me to do."\n            ],\n            \'navigation_request\': [\n                "I\'ll navigate to {location} for you.",\n                "On my way to {location}.",\n                "Navigating to {location} now."\n            ],\n            \'unknown\': [\n                "I\'m not sure I understood. Could you please rephrase?",\n                "I didn\'t catch that. Could you say it again?",\n                "I\'m sorry, I don\'t understand. Can you explain differently?"\n            ]\n        }\n\n    def generate_response(self, intent: str, entities: List[Tuple[str, str]], context: Dict[str, any]) -> str:\n        """Generate appropriate response based on intent and context"""\n        if intent == \'navigation_request\':\n            # Extract location entity\n            location = self._extract_location(entities)\n            if location:\n                template = self.response_templates[\'navigation_request\'][0]\n                return template.format(location=location)\n\n        # Use default template for intent\n        templates = self.response_templates.get(intent, self.response_templates[\'unknown\'])\n        return templates[0] if templates else self.response_templates[\'unknown\'][0]\n\n    def _extract_location(self, entities: List[Tuple[str, str]]) -> str:\n        """Extract location entity from parsed entities"""\n        for entity, label in entities:\n            if label in [\'LOC\', \'GPE\', \'FAC\']:  # Location, geopolitical entity, facility\n                return entity\n        return "the specified location"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"non-verbal-communication",children:"Non-Verbal Communication"}),"\n",(0,s.jsx)(n.h3,{id:"gesture-recognition",children:"Gesture Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Implement gesture-based interaction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nimport mediapipe as mp\nimport numpy as np\n\nclass GestureRecognition:\n    def __init__(self):\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.5\n        )\n        self.mp_drawing = mp.solutions.drawing_utils\n\n    def recognize_gesture(self, image: np.ndarray) -> Dict[str, any]:\n        \"\"\"Recognize gestures from camera image\"\"\"\n        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        results = self.hands.process(rgb_image)\n\n        if results.multi_hand_landmarks:\n            for hand_landmarks in results.multi_hand_landmarks:\n                gesture = self._analyze_hand_pose(hand_landmarks)\n                if gesture:\n                    return {\n                        'gesture': gesture,\n                        'confidence': 0.8,\n                        'landmarks': hand_landmarks\n                    }\n\n        return {'gesture': 'none', 'confidence': 0.0}\n\n    def _analyze_hand_pose(self, landmarks) -> str:\n        \"\"\"Analyze hand landmarks to determine gesture\"\"\"\n        # Calculate distances between key points\n        thumb_tip = landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP]\n        index_tip = landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]\n        middle_tip = landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n        ring_tip = landmarks.landmark[self.mp_hands.HandLandmark.RING_FINGER_TIP]\n        pinky_tip = landmarks.landmark[self.mp_hands.HandLandmark.PINKY_TIP]\n\n        # Simple gesture recognition based on finger positions\n        if self._is_fist(thumb_tip, index_tip, middle_tip, ring_tip, pinky_tip):\n            return 'fist'\n        elif self._is_pointing(index_tip, middle_tip, ring_tip, pinky_tip):\n            return 'pointing'\n        elif self._is_wave(index_tip, middle_tip):\n            return 'wave'\n        else:\n            return 'unknown'\n\n    def _is_fist(self, thumb, index, middle, ring, pinky) -> bool:\n        \"\"\"Check if hand is in fist position\"\"\"\n        # Simplified logic - in practice, use more sophisticated distance calculations\n        return True  # Placeholder\n"})}),"\n",(0,s.jsx)(n.h3,{id:"expressive-motion",children:"Expressive Motion"}),"\n",(0,s.jsx)(n.p,{children:"Create expressive robot movements:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ExpressiveMotion:\n    def __init__(self, robot_interface):\n        self.robot_interface = robot_interface\n        self.expressions = {\n            \'greeting\': self._execute_greeting_motion,\n            \'acknowledgment\': self._execute_acknowledgment_motion,\n            \'confusion\': self._execute_confusion_motion,\n            \'success\': self._execute_success_motion\n        }\n\n    def execute_expression(self, expression_type: str):\n        """Execute appropriate expressive motion"""\n        if expression_type in self.expressions:\n            self.expressions[expression_type]()\n\n    def _execute_greeting_motion(self):\n        """Execute greeting motion"""\n        # Move head/eyes to look at person\n        self.robot_interface.move_head(0, 0.2)  # Look slightly up to appear friendly\n\n        # Wave arm if robot has arms\n        if hasattr(self.robot_interface, \'move_arm\'):\n            self.robot_interface.move_arm(\'wave_pattern\', duration=2.0)\n\n    def _execute_acknowledgment_motion(self):\n        """Execute acknowledgment motion"""\n        # Nod head\n        self.robot_interface.move_head(0.1, 0)  # Small nod\n        time.sleep(0.5)\n        self.robot_interface.move_head(-0.1, 0)  # Return to neutral\n\n    def _execute_confusion_motion(self):\n        """Execute confusion motion"""\n        # Tilt head slightly\n        self.robot_interface.move_head(0, 0.1)  # Tilt\n        time.sleep(0.5)\n        self.robot_interface.move_head(0, -0.1)  # Return to neutral\n'})}),"\n",(0,s.jsx)(n.h2,{id:"social-interaction-patterns",children:"Social Interaction Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"turn-taking",children:"Turn-Taking"}),"\n",(0,s.jsx)(n.p,{children:"Implement natural turn-taking in conversations:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TurnTakingManager:\n    def __init__(self):\n        self.last_speech_time = 0\n        self.human_speaking = False\n        self.robot_speaking = False\n        self.speech_threshold = 0.5  # seconds of silence to trigger turn change\n        self.response_delay = 0.5  # seconds to wait before responding\n\n    def update_speech_status(self, human_speaking: bool):\n        """Update who is currently speaking"""\n        current_time = time.time()\n\n        if human_speaking and not self.human_speaking:\n            # Human just started speaking\n            self.human_speaking = True\n            self.robot_speaking = False\n        elif not human_speaking and self.human_speaking:\n            # Human just stopped speaking\n            self.human_speaking = False\n            self.last_speech_time = current_time\n\n    def should_robot_respond(self) -> bool:\n        """Check if robot should start speaking"""\n        current_time = time.time()\n\n        # Robot can respond if human has stopped speaking for threshold time\n        # and robot is not already speaking\n        return (not self.robot_speaking and\n                not self.human_speaking and\n                (current_time - self.last_speech_time) > self.speech_threshold)\n\n    def robot_start_speaking(self):\n        """Mark robot as starting to speak"""\n        self.robot_speaking = True\n        self.human_speaking = False\n\n    def robot_stop_speaking(self):\n        """Mark robot as finished speaking"""\n        self.robot_speaking = False\n        self.last_speech_time = time.time()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"proxemics",children:"Proxemics"}),"\n",(0,s.jsx)(n.p,{children:"Implement appropriate spatial behavior:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ProxemicsManager:\n    def __init__(self):\n        # Personal space distances in meters\n        self.intimate_distance = 0.45    # 0-1.5 feet\n        self.personal_distance = 1.2     # 1.5-4 feet\n        self.social_distance = 3.6       # 4-12 feet\n        self.public_distance = 7.5       # 12+ feet\n\n        # Current interaction context\n        self.current_interaction = 'casual'  # casual, task, intimate\n\n    def get_appropriate_distance(self, interaction_type: str) -> float:\n        \"\"\"Get appropriate distance based on interaction type\"\"\"\n        distances = {\n            'intimate': self.intimate_distance,\n            'personal': self.personal_distance,\n            'social': self.social_distance,\n            'public': self.public_distance\n        }\n        return distances.get(interaction_type, self.personal_distance)\n\n    def adjust_position_for_interaction(self, human_position: Dict[str, float]):\n        \"\"\"Adjust robot position based on appropriate distance\"\"\"\n        current_distance = self._calculate_distance_to_human(human_position)\n        target_distance = self.get_appropriate_distance(self.current_interaction)\n\n        if current_distance < target_distance * 0.8:  # Too close\n            self._move_away_from_human(human_position, target_distance)\n        elif current_distance > target_distance * 1.2:  # Too far\n            self._move_towards_human(human_position, target_distance)\n\n    def _calculate_distance_to_human(self, human_pos: Dict[str, float]) -> float:\n        \"\"\"Calculate distance to human\"\"\"\n        # Calculate Euclidean distance\n        robot_pos = self.get_robot_position()\n        dx = robot_pos['x'] - human_pos['x']\n        dy = robot_pos['y'] - human_pos['y']\n        return math.sqrt(dx*dx + dy*dy)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"personalization-and-adaptation",children:"Personalization and Adaptation"}),"\n",(0,s.jsx)(n.h3,{id:"user-profiling",children:"User Profiling"}),"\n",(0,s.jsx)(n.p,{children:"Track and adapt to individual users:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class UserProfiler:\n    def __init__(self):\n        self.user_profiles = {}\n        self.interaction_patterns = {}\n\n    def update_user_profile(self, user_id: str, interaction_data: Dict[str, any]):\n        \"\"\"Update user profile based on interaction\"\"\"\n        if user_id not in self.user_profiles:\n            self.user_profiles[user_id] = {\n                'preferences': {},\n                'interaction_style': 'formal',\n                'response_speed_preference': 'normal',\n                'familiarity_level': 0\n            }\n\n        profile = self.user_profiles[user_id]\n\n        # Update preferences based on interaction\n        if 'preferred_greeting' in interaction_data:\n            profile['preferences']['greeting'] = interaction_data['preferred_greeting']\n\n        # Update familiarity level\n        profile['familiarity_level'] = min(1.0, profile['familiarity_level'] + 0.1)\n\n        # Adjust interaction style based on user comfort\n        if self._detect_user_comfort(interaction_data):\n            profile['interaction_style'] = 'casual'\n        else:\n            profile['interaction_style'] = 'formal'\n\n    def get_personalized_response(self, user_id: str, base_response: str) -> str:\n        \"\"\"Get personalized response based on user profile\"\"\"\n        if user_id not in self.user_profiles:\n            return base_response\n\n        profile = self.user_profiles[user_id]\n        familiarity = profile['familiarity_level']\n\n        # Adjust response based on familiarity\n        if familiarity > 0.7:\n            # More casual for familiar users\n            casual_responses = {\n                \"Hello! How can I assist you today?\": \"Hey! What's up? Need help with anything?\",\n                \"I'm ready to help.\": \"Ready when you are!\",\n            }\n            return casual_responses.get(base_response, base_response)\n        else:\n            # More formal for new users\n            return base_response\n\n    def _detect_user_comfort(self, interaction_data: Dict[str, any]) -> bool:\n        \"\"\"Detect if user is comfortable with interaction\"\"\"\n        # In practice, analyze speech patterns, body language, response time, etc.\n        return True  # Placeholder\n"})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-comfort",children:"Safety and Comfort"}),"\n",(0,s.jsx)(n.h3,{id:"safety-protocols",children:"Safety Protocols"}),"\n",(0,s.jsx)(n.p,{children:"Implement safety in human-robot interaction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SafetyInteractionManager:\n    def __init__(self):\n        self.safety_zones = {\n            'danger': 0.5,    # meters - keep humans away\n            'warning': 1.0,   # meters - caution area\n            'safe': 1.5       # meters - safe interaction distance\n        }\n        self.emergency_stop_active = False\n\n    def check_interaction_safety(self, human_position: Dict[str, float]) -> Dict[str, any]:\n        \"\"\"Check if interaction is safe\"\"\"\n        distance = self._calculate_distance_to_human(human_position)\n\n        if distance < self.safety_zones['danger']:\n            return {\n                'safe': False,\n                'zone': 'danger',\n                'action': 'emergency_stop',\n                'message': 'Human too close, stopping immediately'\n            }\n        elif distance < self.safety_zones['warning']:\n            return {\n                'safe': True,\n                'zone': 'warning',\n                'action': 'slow_down',\n                'message': 'Human in warning zone, proceeding with caution'\n            }\n        else:\n            return {\n                'safe': True,\n                'zone': 'safe',\n                'action': 'normal',\n                'message': 'Safe distance maintained'\n            }\n\n    def handle_human_approach(self, approach_speed: float, distance: float):\n        \"\"\"Handle human approaching robot\"\"\"\n        if approach_speed > 1.0 and distance < 1.0:  # Fast approach, close distance\n            self._execute_cautionary_behavior()\n        elif distance < 0.8:\n            self._maintain_safe_distance()\n\n    def _execute_cautionary_behavior(self):\n        \"\"\"Execute behavior when human approaches too quickly\"\"\"\n        # Stop any motion\n        self.stop_robot_motion()\n\n        # Provide audio warning\n        self.speak(\"Please maintain a safe distance\")\n\n    def _maintain_safe_distance(self):\n        \"\"\"Maintain safe distance from human\"\"\"\n        # Move away slowly if necessary\n        pass\n"})}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,s.jsx)(n.h3,{id:"coordinated-interaction",children:"Coordinated Interaction"}),"\n",(0,s.jsx)(n.p,{children:"Coordinate multiple interaction modalities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class MultiModalInteractionManager:\n    def __init__(self, node):\n        self.node = node\n        self.voice_manager = VoiceInteractionManager()\n        self.gesture_manager = GestureRecognition()\n        self.motion_manager = ExpressiveMotion(node.robot_interface)\n        self.turn_taking = TurnTakingManager()\n        self.proxemics = ProxemicsManager()\n        self.user_profiler = UserProfiler()\n        self.safety_manager = SafetyInteractionManager()\n\n        # Publishers and subscribers\n        self.interaction_publisher = node.create_publisher(\n            String, 'interaction_status', 10)\n        self.interaction_subscriber = node.create_subscription(\n            String, 'interaction_command', self.interaction_callback, 10)\n\n    def process_human_interaction(self, sensor_data: Dict[str, any]) -> Dict[str, any]:\n        \"\"\"Process multi-modal human interaction\"\"\"\n        interaction_result = {\n            'understood': False,\n            'response': '',\n            'actions': [],\n            'safety_status': 'safe'\n        }\n\n        # Check safety first\n        safety_check = self.safety_manager.check_interaction_safety(\n            sensor_data.get('human_position', {}))\n\n        if not safety_check['safe']:\n            interaction_result['safety_status'] = safety_check['zone']\n            interaction_result['response'] = \"Safety protocol activated\"\n            return interaction_result\n\n        # Process voice input\n        voice_result = self.voice_manager.listen_and_understand()\n        if 'error' not in voice_result:\n            # Interpret intent\n            intent = voice_result['intent']\n\n            # Generate response\n            response_gen = ContextualResponseGenerator()\n            user_id = sensor_data.get('user_id', 'unknown')\n            response = response_gen.generate_response(\n                intent['type'],\n                intent['entities'],\n                {'context': sensor_data}\n            )\n\n            # Personalize response\n            personalized_response = self.user_profiler.get_personalized_response(\n                user_id, response)\n\n            interaction_result['response'] = personalized_response\n            interaction_result['understood'] = True\n\n            # Execute appropriate actions\n            if intent['type'] == 'greeting':\n                self.motion_manager.execute_expression('greeting')\n            elif intent['type'] == 'request_help':\n                self.motion_manager.execute_expression('acknowledgment')\n\n        return interaction_result\n\n    def interaction_callback(self, msg: String):\n        \"\"\"Handle interaction commands\"\"\"\n        try:\n            command_data = json.loads(msg.data)\n            result = self.process_interaction_command(command_data)\n\n            # Publish interaction status\n            status_msg = String()\n            status_msg.data = json.dumps(result)\n            self.interaction_publisher.publish(status_msg)\n\n        except json.JSONDecodeError:\n            self.node.get_logger().error(f\"Invalid interaction command: {msg.data}\")\n"})}),"\n",(0,s.jsx)(n.h2,{id:"cultural-considerations",children:"Cultural Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"cultural-adaptation",children:"Cultural Adaptation"}),"\n",(0,s.jsx)(n.p,{children:"Adapt interaction to cultural preferences:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class CulturalInteractionAdapter:\n    def __init__(self):\n        self.cultural_settings = {\n            'japan': {\n                'bow_greeting': True,\n                'formal_language': True,\n                'respectful_distance': 1.5,\n                'eye_contact': 'moderate'\n            },\n            'usa': {\n                'handshake_greeting': True,\n                'casual_language': True,\n                'personal_distance': 1.2,\n                'eye_contact': 'high'\n            },\n            'middle_east': {\n                'respectful_gestures': True,\n                'formal_addressing': True,\n                'gender_considerations': True,\n                'distance_preferences': 1.0\n            }\n        }\n\n    def adapt_interaction_for_culture(self, culture: str, base_interaction: Dict[str, any]) -> Dict[str, any]:\n        \"\"\"Adapt interaction based on cultural preferences\"\"\"\n        if culture.lower() not in self.cultural_settings:\n            return base_interaction\n\n        culture_settings = self.cultural_settings[culture.lower()]\n        adapted_interaction = base_interaction.copy()\n\n        # Adjust greeting based on culture\n        if culture_settings.get('bow_greeting'):\n            adapted_interaction['greeting_motion'] = 'bow'\n        elif culture_settings.get('handshake_greeting'):\n            adapted_interaction['greeting_motion'] = 'handshake'\n\n        # Adjust language formality\n        if culture_settings.get('formal_language'):\n            adapted_interaction['language_level'] = 'formal'\n        else:\n            adapted_interaction['language_level'] = 'casual'\n\n        # Adjust personal space\n        if 'respectful_distance' in culture_settings:\n            adapted_interaction['personal_space'] = culture_settings['respectful_distance']\n\n        return adapted_interaction\n"})}),"\n",(0,s.jsx)(n.h2,{id:"privacy-and-ethics",children:"Privacy and Ethics"}),"\n",(0,s.jsx)(n.h3,{id:"privacy-protection",children:"Privacy Protection"}),"\n",(0,s.jsx)(n.p,{children:"Implement privacy considerations in HRI:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class PrivacyAwareInteraction:\n    def __init__(self):\n        self.data_retention_policy = {\n            \'conversation_logs\': 30,  # days\n            \'user_profiles\': 365,     # days\n            \'biometric_data\': 7       # days\n        }\n        self.consent_status = {}  # Track user consent for data collection\n\n    def collect_interaction_data(self, user_id: str, data_type: str, data: any) -> bool:\n        """Collect interaction data with privacy considerations"""\n        if not self.check_consent(user_id, data_type):\n            return False\n\n        # Anonymize data where possible\n        if data_type == \'conversation\':\n            anonymized_data = self.anonymize_conversation(data)\n        else:\n            anonymized_data = data\n\n        # Store with retention policy\n        self.store_data_with_retention(user_id, data_type, anonymized_data)\n        return True\n\n    def check_consent(self, user_id: str, data_type: str) -> bool:\n        """Check if user has consented to data collection"""\n        if user_id not in self.consent_status:\n            return False\n\n        consent = self.consent_status[user_id]\n        return consent.get(data_type, False) or consent.get(\'general\', False)\n\n    def anonymize_conversation(self, conversation: str) -> str:\n        """Anonymize personal information from conversation"""\n        # Remove or obfuscate personal information\n        import re\n        # Replace names, addresses, etc. with generic placeholders\n        anonymized = re.sub(r\'\\b[A-Z][a-z]+\\b\', \'[PERSON]\', conversation)\n        return anonymized\n'})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-and-metrics",children:"Evaluation and Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"interaction-quality-metrics",children:"Interaction Quality Metrics"}),"\n",(0,s.jsx)(n.p,{children:"Evaluate HRI system performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class HRIQualityEvaluator:\n    def __init__(self):\n        self.metrics = {\n            'interaction_success_rate': [],\n            'user_satisfaction': [],\n            'response_time': [],\n            'naturalness_score': [],\n            'safety_incidents': []\n        }\n\n    def evaluate_interaction(self, interaction_data: Dict[str, any]) -> Dict[str, float]:\n        \"\"\"Evaluate quality of interaction\"\"\"\n        metrics = {}\n\n        # Success rate: Did the interaction achieve its goal?\n        metrics['success'] = self._calculate_success_rate(interaction_data)\n\n        # Response time: How quickly did robot respond?\n        metrics['responsiveness'] = self._calculate_responsiveness(interaction_data)\n\n        # Naturalness: How natural did the interaction feel?\n        metrics['naturalness'] = self._calculate_naturalness(interaction_data)\n\n        # Safety: Was the interaction conducted safely?\n        metrics['safety'] = self._calculate_safety_score(interaction_data)\n\n        # Store metrics for long-term evaluation\n        self._store_metrics(metrics)\n\n        return metrics\n\n    def _calculate_success_rate(self, data: Dict[str, any]) -> float:\n        \"\"\"Calculate interaction success rate\"\"\"\n        # Based on whether goals were achieved\n        return data.get('goals_achieved', 0) / max(data.get('total_goals', 1), 1)\n\n    def _calculate_responsiveness(self, data: Dict[str, any]) -> float:\n        \"\"\"Calculate responsiveness score\"\"\"\n        avg_response_time = data.get('avg_response_time', float('inf'))\n        # Score from 0-1, where 1 is very responsive (fast response)\n        if avg_response_time == float('inf'):\n            return 0.0\n        return max(0.0, min(1.0, 2.0 / (avg_response_time + 1)))  # Normalize\n\n    def _calculate_naturalness(self, data: Dict[str, any]) -> float:\n        \"\"\"Calculate naturalness of interaction\"\"\"\n        # Based on user feedback, turn-taking smoothness, etc.\n        user_ratings = data.get('user_ratings', [])\n        if user_ratings:\n            return sum(user_ratings) / len(user_ratings)\n        return 0.5  # Default neutral score\n\n    def _calculate_safety_score(self, data: Dict[str, any]) -> float:\n        \"\"\"Calculate safety score\"\"\"\n        incidents = data.get('safety_incidents', 0)\n        # Higher score means fewer incidents\n        return max(0.0, 1.0 - incidents * 0.1)  # Deduct 0.1 per incident\n\n    def get_overall_hri_performance(self) -> Dict[str, float]:\n        \"\"\"Get overall HRI system performance\"\"\"\n        if not self.metrics['interaction_success_rate']:\n            return {'message': 'No interaction data available'}\n\n        return {\n            'average_success_rate': sum(self.metrics['interaction_success_rate']) / len(self.metrics['interaction_success_rate']),\n            'average_satisfaction': sum(self.metrics['user_satisfaction']) / len(self.metrics['user_satisfaction']),\n            'average_response_time': sum(self.metrics['response_time']) / len(self.metrics['response_time']),\n            'average_naturalness': sum(self.metrics['naturalness_score']) / len(self.metrics['naturalness_score']),\n            'total_interactions': len(self.metrics['interaction_success_rate'])\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Misunderstanding"}),": Robot not understanding user input"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Inappropriate responses"}),": Responses not matching context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety violations"}),": Robot not maintaining safe distances"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cultural insensitivity"}),": Not adapting to cultural preferences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy concerns"}),": Improper handling of personal data"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"solutions",children:"Solutions"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context awareness"}),": Improve understanding of interaction context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response validation"}),": Validate responses before delivery"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety layers"}),": Multiple safety validation steps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cultural databases"}),": Maintain cultural preference databases"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy by design"}),": Build privacy into system architecture"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"This concludes Module 4 on Vision-Language-Action systems. You now have a comprehensive understanding of how humanoid robots can understand natural language, perceive their environment, and execute complex tasks while maintaining natural interaction with humans. Continue to the Capstone module to integrate all components into a complete autonomous humanoid system."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var s=t(6540);const a={},i=s.createContext(a);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);